{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c8ac0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('/n/groups/marks/users/courtney/projects/regulatory_genomics/models/LOL-EVE/data/benchmark_data/gnomad_ultra_rare_full.csv')\n",
    "\n",
    "# Define indel cuts\n",
    "indel_cuts = {\n",
    "    'Small Indel (≤2bp)': df.indel_length <= 2,\n",
    "    'Medium Indel (3-15bp)': (df.indel_length >= 3) & (df.indel_length <= 15),\n",
    "    'Large Indel (16-50bp)': (df.indel_length > 15) & (df.indel_length <= 50),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a51b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_ratio(ultra, common):\n",
    "    \"\"\"Safely calculate ratio, handling edge cases\"\"\"\n",
    "    if common == 0 or np.isnan(common) or np.isnan(ultra):\n",
    "        return 1.0\n",
    "    ratio = ultra / common\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        return 1.0\n",
    "    return ratio\n",
    "\n",
    "def calculate_detailed_counts(df, models, percentile_thresholds):\n",
    "    \"\"\"Calculate detailed counts including variants and unique genes for each model and percentile\"\"\"\n",
    "    detailed_counts = {}\n",
    "    \n",
    "    for model in models:\n",
    "        # Skip if model column doesn't exist\n",
    "        if model not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        detailed_counts[model] = {'ultra_rare': {}, 'common': {}}\n",
    "        \n",
    "        # Get data for ultra-rare and common variants (non-NaN values)\n",
    "        ultra_data = df[df.ultra_rare & df[model].notna()]\n",
    "        common_data = df[df.common & df[model].notna()]\n",
    "        \n",
    "        # Store total counts\n",
    "        detailed_counts[model]['ultra_rare']['total_variants'] = len(ultra_data)\n",
    "        detailed_counts[model]['ultra_rare']['total_genes'] = ultra_data['GENE'].nunique()\n",
    "        detailed_counts[model]['common']['total_variants'] = len(common_data)\n",
    "        detailed_counts[model]['common']['total_genes'] = common_data['GENE'].nunique()\n",
    "        \n",
    "        # For each percentile threshold\n",
    "        for p in percentile_thresholds:\n",
    "            if len(ultra_data) > 0:\n",
    "                ultra_threshold = np.percentile(ultra_data[model], p)\n",
    "                # Count variants AT OR BELOW the percentile threshold (most deleterious = lowest scores)\n",
    "                ultra_subset = ultra_data[ultra_data[model] <= ultra_threshold]\n",
    "                detailed_counts[model]['ultra_rare'][p] = {\n",
    "                    'variants': len(ultra_subset),\n",
    "                    'genes': ultra_subset['GENE'].nunique()\n",
    "                }\n",
    "            else:\n",
    "                detailed_counts[model]['ultra_rare'][p] = {'variants': 0, 'genes': 0}\n",
    "                \n",
    "            if len(common_data) > 0:\n",
    "                common_threshold = np.percentile(common_data[model], p)\n",
    "                # Count variants AT OR BELOW the percentile threshold (most deleterious = lowest scores)\n",
    "                common_subset = common_data[common_data[model] <= common_threshold]\n",
    "                detailed_counts[model]['common'][p] = {\n",
    "                    'variants': len(common_subset),\n",
    "                    'genes': common_subset['GENE'].nunique()\n",
    "                }\n",
    "            else:\n",
    "                detailed_counts[model]['common'][p] = {'variants': 0, 'genes': 0}\n",
    "    \n",
    "    return detailed_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1a9a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_ratios_with_errors(df, models, percentile_thresholds, bins=10):\n",
    "    \"\"\"Calculate ratios and errors for each model weighted by indel length distribution\"\"\"\n",
    "    # Initialize data structures for weighted ratios and errors\n",
    "    weighted_ratios = {}\n",
    "    weighted_errors = {}\n",
    "    detailed_counts = calculate_detailed_counts(df, models, percentile_thresholds)\n",
    "    \n",
    "    # Create bins based on the indel length distribution\n",
    "    indel_min = df.indel_length.min()\n",
    "    indel_max = df.indel_length.max()\n",
    "    \n",
    "    # Use logarithmic bins to handle the skewed distribution\n",
    "    bin_edges = np.logspace(np.log10(max(1, indel_min)), np.log10(indel_max), bins+1)\n",
    "    bin_centers = np.sqrt(bin_edges[:-1] * bin_edges[1:])\n",
    "    \n",
    "    # Calculate indel length weights (probability of each bin) - ORIGINAL METHOD\n",
    "    hist, _ = np.histogram(df.indel_length, bins=bin_edges, density=True)\n",
    "    weights = hist / hist.sum()  # Normalize to sum to 1\n",
    "    \n",
    "    # Initialize data structures for collecting raw ratios from each bin\n",
    "    bin_ratios = {model: {p: [] for p in percentile_thresholds} for model in models if model in df.columns}\n",
    "    bin_weights = {model: {p: [] for p in percentile_thresholds} for model in models if model in df.columns}\n",
    "    \n",
    "    # For each bin, calculate ratios and weights\n",
    "    for i in range(bins):\n",
    "        # Get variants in this bin\n",
    "        if i < bins - 1:\n",
    "            bin_df = df[(df.indel_length >= bin_edges[i]) & (df.indel_length < bin_edges[i+1])]\n",
    "        else:\n",
    "            bin_df = df[(df.indel_length >= bin_edges[i]) & (df.indel_length <= bin_edges[i+1])]\n",
    "        \n",
    "        # Only proceed if we have enough data in this bin\n",
    "        if len(bin_df) < 10:  # Skip bins with too few variants\n",
    "            continue\n",
    "        \n",
    "        # For each model and percentile, calculate ratios\n",
    "        for model in models:\n",
    "            if model not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            if model not in weighted_ratios:\n",
    "                weighted_ratios[model] = {p: 0.0 for p in percentile_thresholds}\n",
    "                weighted_errors[model] = {p: 0.0 for p in percentile_thresholds}\n",
    "            \n",
    "            for p in percentile_thresholds:\n",
    "                # Get data for ultra-rare and common variants\n",
    "                ultra_data = bin_df[bin_df.ultra_rare][model].dropna()\n",
    "                common_data = bin_df[bin_df.common][model].dropna()\n",
    "                \n",
    "                if len(ultra_data) > 0 and len(common_data) > 0:\n",
    "                    # Calculate percentiles - MATCH ORIGINAL (even though it's technically wrong)\n",
    "                    ultra = np.percentile(ultra_data, p)\n",
    "                    common = np.percentile(common_data, p)\n",
    "                    \n",
    "                    # Calculate ratio\n",
    "                    ratio = safe_ratio(ultra, common)\n",
    "                    \n",
    "                    # Store raw ratio and corresponding weight for error calculation\n",
    "                    bin_ratios[model][p].append(ratio)\n",
    "                    bin_weights[model][p].append(weights[i])\n",
    "                    \n",
    "                    # Calculate weighted ratio\n",
    "                    weighted_ratio = ratio * weights[i]\n",
    "                    \n",
    "                    # Add to the weighted sum\n",
    "                    weighted_ratios[model][p] += weighted_ratio\n",
    "    \n",
    "    # Calculate weighted standard errors\n",
    "    for model in weighted_ratios:\n",
    "        for p in percentile_thresholds:\n",
    "            if len(bin_ratios[model][p]) > 1:\n",
    "                # Calculate weighted standard error\n",
    "                bin_values = np.array(bin_ratios[model][p])\n",
    "                bin_w = np.array(bin_weights[model][p])\n",
    "                bin_w = bin_w / bin_w.sum()  # Normalize weights\n",
    "                \n",
    "                # Weighted mean\n",
    "                weighted_mean = np.average(bin_values, weights=bin_w)\n",
    "                \n",
    "                # Weighted variance\n",
    "                variance = np.average((bin_values - weighted_mean)**2, weights=bin_w)\n",
    "                \n",
    "                # Standard error (divide by sqrt of effective sample size)\n",
    "                n_effective = 1.0 / np.sum(bin_w**2)  # Effective sample size for weighted data\n",
    "                std_error = np.sqrt(variance / n_effective)\n",
    "                \n",
    "                weighted_errors[model][p] = std_error\n",
    "            else:\n",
    "                weighted_errors[model][p] = 0.0\n",
    "    \n",
    "    return weighted_ratios, weighted_errors, detailed_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fab5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_statistics(weighted_ratios, model):\n",
    "    \"\"\"Calculate summary statistics across percentiles for a model\"\"\"\n",
    "    ratios = list(weighted_ratios[model].values())\n",
    "    return {\n",
    "        'mean': np.mean(ratios),\n",
    "        'median': np.median(ratios),\n",
    "        'std': np.std(ratios),\n",
    "        'variance': np.var(ratios),\n",
    "        'stderr': np.std(ratios) / np.sqrt(len(ratios))\n",
    "    }\n",
    "\n",
    "def create_summary_table(df, models, name_mapping, percentile_thresholds):\n",
    "    \"\"\"Create a comprehensive summary table of all results\"\"\"\n",
    "    \n",
    "    # Store results for each cut\n",
    "    all_results = {}\n",
    "    \n",
    "    for cut_name, cut_mask in indel_cuts.items():\n",
    "        df_cut = df[cut_mask].copy()\n",
    "        df_cut['ultra_rare'] = df_cut.MAF < 0.00001\n",
    "        df_cut['common'] = df_cut.MAF >= 0.001\n",
    "        \n",
    "        # Calculate weighted ratios\n",
    "        weighted_ratios, weighted_errors, detailed_counts = calculate_weighted_ratios_with_errors(\n",
    "            df_cut, models, percentile_thresholds, bins=10\n",
    "        )\n",
    "        \n",
    "        # Calculate summary statistics for each model\n",
    "        cut_results = {}\n",
    "        for model in models:\n",
    "            if model in weighted_ratios:\n",
    "                cut_results[model] = calculate_summary_statistics(weighted_ratios, model)\n",
    "        \n",
    "        all_results[cut_name] = cut_results\n",
    "    \n",
    "    # Prepare data for the table\n",
    "    table_data = []\n",
    "    \n",
    "    for cut_name, cut_results in all_results.items():\n",
    "        for model, stats in cut_results.items():\n",
    "            # Get clean model name\n",
    "            model_name = name_mapping.get(model, model)\n",
    "            \n",
    "            table_data.append({\n",
    "                'Indel_Category': cut_name,\n",
    "                'Model': model_name,\n",
    "                'Mean_Ratio': f\"{stats['mean']:.3f}\",\n",
    "                'Median_Ratio': f\"{stats['median']:.3f}\",\n",
    "                'Std_Dev': f\"{stats['std']:.3f}\",\n",
    "                'Std_Error': f\"{stats['stderr']:.3f}\",\n",
    "                'Variance': f\"{stats['variance']:.3f}\"\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Sort by category and mean ratio\n",
    "    summary_df['Mean_Ratio_Numeric'] = summary_df['Mean_Ratio'].astype(float)\n",
    "    summary_df = summary_df.sort_values(['Indel_Category', 'Mean_Ratio_Numeric'], ascending=[True, False])\n",
    "    summary_df = summary_df.drop('Mean_Ratio_Numeric', axis=1)\n",
    "    \n",
    "    return summary_df, all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cec54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detailed_counts_table(df, models, name_mapping, percentile_thresholds):\n",
    "    \"\"\"Create a detailed table showing counts at each percentile for each model and indel cut\"\"\"\n",
    "    \n",
    "    # Collect all count data\n",
    "    all_count_data = []\n",
    "    \n",
    "    for cut_name, cut_mask in indel_cuts.items():\n",
    "        df_cut = df[cut_mask].copy()\n",
    "        df_cut['ultra_rare'] = df_cut.MAF < 0.00001\n",
    "        df_cut['common'] = df_cut.MAF >= 0.001\n",
    "        \n",
    "        # Calculate detailed counts for this cut\n",
    "        detailed_counts = calculate_detailed_counts(df_cut, models, percentile_thresholds)\n",
    "        \n",
    "        for model in models:\n",
    "            if model not in detailed_counts:\n",
    "                continue\n",
    "                \n",
    "            model_name = name_mapping.get(model, model)\n",
    "            \n",
    "            # Add total counts\n",
    "            all_count_data.append({\n",
    "                'Indel_Category': cut_name,\n",
    "                'Model': model_name,\n",
    "                'Percentile': 'Total',\n",
    "                'Ultra_Rare_Variants': detailed_counts[model]['ultra_rare']['total_variants'],\n",
    "                'Ultra_Rare_Genes': detailed_counts[model]['ultra_rare']['total_genes'],\n",
    "                'Common_Variants': detailed_counts[model]['common']['total_variants'],\n",
    "                'Common_Genes': detailed_counts[model]['common']['total_genes']\n",
    "            })\n",
    "            \n",
    "            # Add percentile-specific counts\n",
    "            for p in percentile_thresholds:\n",
    "                all_count_data.append({\n",
    "                    'Indel_Category': cut_name,\n",
    "                    'Model': model_name,\n",
    "                    'Percentile': f'{p*100:.1f}%',\n",
    "                    'Ultra_Rare_Variants': detailed_counts[model]['ultra_rare'][p]['variants'],\n",
    "                    'Ultra_Rare_Genes': detailed_counts[model]['ultra_rare'][p]['genes'],\n",
    "                    'Common_Variants': detailed_counts[model]['common'][p]['variants'],\n",
    "                    'Common_Genes': detailed_counts[model]['common'][p]['genes']\n",
    "                })\n",
    "    \n",
    "    counts_df = pd.DataFrame(all_count_data)\n",
    "    return counts_df\n",
    "\n",
    "def create_summary_counts_table(counts_df):\n",
    "    \"\"\"Create a summary table of counts that's easier to read\"\"\"\n",
    "    \n",
    "    # Create a more readable summary for totals\n",
    "    summary_data = []\n",
    "    \n",
    "    for cut in counts_df['Indel_Category'].unique():\n",
    "        for model in counts_df['Model'].unique():\n",
    "            cut_model_data = counts_df[(counts_df['Indel_Category'] == cut) & (counts_df['Model'] == model)]\n",
    "            \n",
    "            if len(cut_model_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get total counts\n",
    "            ultra_total = cut_model_data[(cut_model_data['Percentile'] == 'Total')]\n",
    "            \n",
    "            if len(ultra_total) > 0:\n",
    "                summary_data.append({\n",
    "                    'Indel_Category': cut,\n",
    "                    'Model': model,\n",
    "                    'Ultra_Rare_Variants_Total': ultra_total.iloc[0]['Ultra_Rare_Variants'],\n",
    "                    'Ultra_Rare_Genes_Total': ultra_total.iloc[0]['Ultra_Rare_Genes'],\n",
    "                    'Common_Variants_Total': ultra_total.iloc[0]['Common_Variants'],\n",
    "                    'Common_Genes_Total': ultra_total.iloc[0]['Common_Genes'],\n",
    "                    'Variant_Ratio_UR_to_Common': (ultra_total.iloc[0]['Ultra_Rare_Variants'] / \n",
    "                                                  max(1, ultra_total.iloc[0]['Common_Variants'])),\n",
    "                    'Gene_Ratio_UR_to_Common': (ultra_total.iloc[0]['Ultra_Rare_Genes'] / \n",
    "                                              max(1, ultra_total.iloc[0]['Common_Genes']))\n",
    "                })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c0c46d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CSV tables...\n",
      "Creating summary statistics table...\n",
      "✓ Summary table saved: 'workshop_summary_table.csv'\n",
      "Creating detailed counts table...\n",
      "✓ Detailed counts table saved: 'workshop_detailed_counts.csv'\n",
      "Creating summary counts table...\n",
      "✓ Summary counts table saved: 'workshop_counts_summary.csv'\n",
      "Creating percentile counts table...\n",
      "✓ Percentile counts table saved: 'workshop_percentile_counts.csv'\n",
      "\n",
      "============================================================\n",
      "ALL CSV TABLES GENERATED SUCCESSFULLY!\n",
      "============================================================\n",
      "Files created:\n",
      "1. workshop_summary_table.csv - Summary statistics (mean, median, std, etc.)\n",
      "2. workshop_detailed_counts.csv - Detailed variant/gene counts by percentile\n",
      "3. workshop_counts_summary.csv - Summary of total counts and ratios\n",
      "4. workshop_percentile_counts.csv - Same as detailed counts (for compatibility)\n",
      "\n",
      "Sample of summary table:\n",
      "           Indel_Category                 Model Mean_Ratio Median_Ratio  \\\n",
      "58  Large Indel (16-50bp)                  CADD      2.122        2.123   \n",
      "59  Large Indel (16-50bp)               LOL-EVE      2.096        2.042   \n",
      "66  Large Indel (16-50bp)             speciesLM      1.739        1.743   \n",
      "67  Large Indel (16-50bp)          gpn_promoter      1.490        1.511   \n",
      "46  Large Indel (16-50bp)         HyenaDNA-tiny      1.368        1.353   \n",
      "60  Large Indel (16-50bp)                PhyloP      1.344        1.335   \n",
      "54  Large Indel (16-50bp)         NT-2.5b-multi      1.330        1.304   \n",
      "51  Large Indel (16-50bp)           Caduceus-ph      1.316        1.313   \n",
      "52  Large Indel (16-50bp)           Caduceus-ps      1.310        1.275   \n",
      "47  Large Indel (16-50bp)  HyenaDNA-medium-450k      1.307        1.305   \n",
      "\n",
      "   Std_Dev Std_Error Variance  \n",
      "58   0.027     0.013    0.001  \n",
      "59   0.218     0.109    0.047  \n",
      "66   0.059     0.030    0.004  \n",
      "67   0.147     0.073    0.022  \n",
      "46   0.082     0.041    0.007  \n",
      "60   0.053     0.027    0.003  \n",
      "54   0.082     0.041    0.007  \n",
      "51   0.322     0.161    0.104  \n",
      "52   0.251     0.125    0.063  \n",
      "47   0.076     0.038    0.006  \n"
     ]
    }
   ],
   "source": [
    "# Define models and parameters\n",
    "models = [\n",
    "    'mean_cross_entropy_diff_hyenadna-tiny-1k-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-450k-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-160k-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-large-1m-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-small-32k-seqlen',\n",
    "    'mean_cross_entropy_diff_caduceus-ph_seqlen-131k_d_model-256_n_layer-16',\n",
    "    'mean_cross_entropy_diff_caduceus-ps_seqlen-131k_d_model-256_n_layer-16',\n",
    "    'mean_cross_entropy_diff_DNABERT-2-117M',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-multi-species',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-1000g',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-500m-human-ref',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-v2-500m-multi-species',\n",
    "    'CADD_raw_score',\n",
    "    'LOL-EVE_AF',\n",
    "    'PhyloP',\n",
    "    'mean_cross_entropy_diff_convnet',\n",
    "    'GC_percentage_delta',\n",
    "    'dist_tss',\n",
    "    'max_track_single',\n",
    "    'mean_diff_evo_1_131k_base',\n",
    "    'mean_cross_entropy_diff_johahi/specieslm-metazoa-upstream-k6',\n",
    "    'mean_cross_entropy_diff_songlab/gpn-animal-promoter',\n",
    "    'mean_cross_entropy_diff_evo2-7b'\n",
    "]\n",
    "\n",
    "name_mapping = {\n",
    "    'mean_cross_entropy_diff_hyenadna-tiny-1k-seqlen': 'HyenaDNA-tiny',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-450k-seqlen': 'HyenaDNA-medium-450k',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-160k-seqlen': 'HyenaDNA-medium-160k',\n",
    "    'mean_cross_entropy_diff_hyenadna-large-1m-seqlen': 'HyenaDNA-large',\n",
    "    'mean_cross_entropy_diff_hyenadna-small-32k-seqlen': 'HyenaDNA-small',\n",
    "    'mean_cross_entropy_diff_caduceus-ph_seqlen-131k_d_model-256_n_layer-16': 'Caduceus-ph',\n",
    "    'mean_cross_entropy_diff_caduceus-ps_seqlen-131k_d_model-256_n_layer-16': 'Caduceus-ps',\n",
    "    'mean_cross_entropy_diff_DNABERT-2-117M': 'DNABERT-2',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-multi-species': 'NT-2.5b-multi',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-1000g': 'NT-2.5b-1000g',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-500m-human-ref': 'NT-500m',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-v2-500m-multi-species': 'NT-v2-500m',\n",
    "    'CADD_raw_score': 'CADD',\n",
    "    'LOL-EVE_AF': 'LOL-EVE',\n",
    "    'PhyloP': 'PhyloP',\n",
    "    'mean_cross_entropy_diff_convnet': 'GPN',\n",
    "    'GC_percentage_delta': 'GC Content Δ',\n",
    "    'mean_cross_entropy_diff_johahi/specieslm-metazoa-upstream-k6': 'speciesLM',\n",
    "    'mean_cross_entropy_diff_songlab/gpn-animal-promoter': 'gpn_promoter',\n",
    "    'mean_cross_entropy_diff_evo2-7b': 'evo2',\n",
    "    'mean_diff_evo_1_131k_base': 'evo1',\n",
    "    'max_track_single': 'Enformer',\n",
    "    'dist_tss':'Distance TSS'\n",
    "}\n",
    "\n",
    "percentile_thresholds = np.array([0.01, 0.025, 0.05, 0.1])\n",
    "\n",
    "# Ensure required columns exist\n",
    "df['ultra_rare'] = df.MAF < 0.00001\n",
    "df['common'] = df.MAF >= 0.001\n",
    "\n",
    "print(\"Generating CSV tables...\")\n",
    "\n",
    "# Generate summary statistics table\n",
    "print(\"Creating summary statistics table...\")\n",
    "summary_table, summary_stats = create_summary_table(df, models, name_mapping, percentile_thresholds)\n",
    "summary_table.to_csv('workshop_summary_table.csv', index=False)\n",
    "print(\"✓ Summary table saved: 'workshop_summary_table.csv'\")\n",
    "\n",
    "# Generate detailed counts table\n",
    "print(\"Creating detailed counts table...\")\n",
    "detailed_counts_table = create_detailed_counts_table(df, models, name_mapping, percentile_thresholds)\n",
    "detailed_counts_table.to_csv('workshop_detailed_counts.csv', index=False)\n",
    "print(\"✓ Detailed counts table saved: 'workshop_detailed_counts.csv'\")\n",
    "\n",
    "# Generate summary counts table\n",
    "print(\"Creating summary counts table...\")\n",
    "summary_counts_table = create_summary_counts_table(detailed_counts_table)\n",
    "summary_counts_table.to_csv('workshop_counts_summary.csv', index=False)\n",
    "print(\"✓ Summary counts table saved: 'workshop_counts_summary.csv'\")\n",
    "\n",
    "# Generate percentile counts table (same as detailed but with different name for clarity)\n",
    "print(\"Creating percentile counts table...\")\n",
    "percentile_counts_table = detailed_counts_table.copy()\n",
    "percentile_counts_table.to_csv('workshop_percentile_counts.csv', index=False)\n",
    "print(\"✓ Percentile counts table saved: 'workshop_percentile_counts.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL CSV TABLES GENERATED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files created:\")\n",
    "print(\"1. workshop_summary_table.csv - Summary statistics (mean, median, std, etc.)\")\n",
    "print(\"2. workshop_detailed_counts.csv - Detailed variant/gene counts by percentile\")\n",
    "print(\"3. workshop_counts_summary.csv - Summary of total counts and ratios\")\n",
    "print(\"4. workshop_percentile_counts.csv - Same as detailed counts (for compatibility)\")\n",
    "\n",
    "# Display sample of summary table\n",
    "print(\"\\nSample of summary table:\")\n",
    "print(summary_table.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd1819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courtney_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
