{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from functools import lru_cache\n",
    "import os\n",
    "\n",
    "# Set display options and plotting style\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = './diagonal_constraint_analysis'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# File paths - update these to your locations\n",
    "TFBS_RESULTS_PATH = ''\n",
    "OLD_TFBS_PATH = ''  # Contains TF information\n",
    "ORTHODB_RATES_PATH = ''\n",
    "GTEX_FILE = \"\"\n",
    "BIOMART_PATH = \"\"\n",
    "TRAINING_GENES_PATH = ''\n",
    "\n",
    "# Define model columns based on what's actually available in the data\n",
    "model_columns = [\n",
    "    # HyenaDNA models\n",
    "    'mean_cross_entropy_diff_hyenadna-tiny-1k-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-450k-seqlen', \n",
    "    'mean_cross_entropy_diff_hyenadna-medium-160k-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-large-1m-seqlen',\n",
    "    'mean_cross_entropy_diff_hyenadna-small-32k-seqlen', \n",
    "    \n",
    "    # Other transformer models\n",
    "    'mean_cross_entropy_diff_DNABERT-2-117M',\n",
    "    'mean_cross_entropy_diff_caduceus-ph_seqlen-131k_d_model-256_n_layer-16',\n",
    "    'mean_cross_entropy_diff_caduceus-ps_seqlen-131k_d_model-256_n_layer-16',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-multi-species',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-1000g',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-500m-human-ref',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-v2-500m-multi-species',\n",
    "    \n",
    "    # Existing named columns in old data\n",
    "    'GPN',\n",
    "    'Phylop', \n",
    "    'LOL-EVE',  # This exists in old data\n",
    "    'Enformer', # This exists in old data\n",
    "    \n",
    "    # Other models\n",
    "    'mean_diff_evo_1_131k_base',\n",
    "    'mean_cross_entropy_diff_johahi/specieslm-metazoa-upstream-k6',\n",
    "    'mean_cross_entropy_diff_evo2-7b',\n",
    "    'mean_cross_entropy_diff_songlab/gpn-animal-promoter'\n",
    "    \n",
    "    # Note: Excluded LOL-EVE ablation columns to focus on main models only\n",
    "]\n",
    "\n",
    "# Color and display name mappings\n",
    "model_colors = {\n",
    "    'LOL-EVE': '#00aa55',\n",
    "    'Enformer': '#9467bd',\n",
    "    'GPN': '#ffbb78',\n",
    "    'Phylop': '#ff7f0e',\n",
    "    \n",
    "    # HyenaDNA family - shades of blue/teal\n",
    "    'mean_cross_entropy_diff_hyenadna-tiny-1k-seqlen': '#17becf',\n",
    "    'mean_cross_entropy_diff_hyenadna-small-32k-seqlen': '#14a3c7',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-160k-seqlen': '#1190b8',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-450k-seqlen': '#0e7ca8',\n",
    "    'mean_cross_entropy_diff_hyenadna-large-1m-seqlen': '#0b6999',\n",
    "    \n",
    "    # Caduceus family - shades of pink/magenta\n",
    "    'mean_cross_entropy_diff_caduceus-ph_seqlen-131k_d_model-256_n_layer-16': '#e377c2',\n",
    "    'mean_cross_entropy_diff_caduceus-ps_seqlen-131k_d_model-256_n_layer-16': '#d85fb8',\n",
    "    \n",
    "    # Nucleotide transformer family - shades of brown/tan\n",
    "    'mean_cross_entropy_diff_DNABERT-2-117M': '#8c564b',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-multi-species': '#a0784f',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-1000g': '#b49a53',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-500m-human-ref': '#c8bc57',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-v2-500m-multi-species': '#dcde5b',\n",
    "    \n",
    "    # Other models\n",
    "    'mean_cross_entropy_diff_evo2-7b': '#ff9896',\n",
    "    'mean_cross_entropy_diff_johahi/specieslm-metazoa-upstream-k6': '#9edae5',\n",
    "    'mean_cross_entropy_diff_songlab/gpn-animal-promoter': '#2ca02c',\n",
    "    'mean_diff_evo_1_131k_base': '#ffbb78'\n",
    "}\n",
    "\n",
    "model_display_names = {\n",
    "    'LOL-EVE': 'LOL-EVE',\n",
    "    'Enformer': 'Enformer', \n",
    "    'GPN': 'GPN',\n",
    "    'Phylop': 'PhyloP',\n",
    "    \n",
    "    # HyenaDNA family\n",
    "    'mean_cross_entropy_diff_hyenadna-tiny-1k-seqlen': 'HyenaDNA-Tiny',\n",
    "    'mean_cross_entropy_diff_hyenadna-small-32k-seqlen': 'HyenaDNA-Small',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-160k-seqlen': 'HyenaDNA-Medium-160k',\n",
    "    'mean_cross_entropy_diff_hyenadna-medium-450k-seqlen': 'HyenaDNA-Medium-450k',\n",
    "    'mean_cross_entropy_diff_hyenadna-large-1m-seqlen': 'HyenaDNA-Large',\n",
    "    \n",
    "    # Other transformers\n",
    "    'mean_cross_entropy_diff_DNABERT-2-117M': 'DNABERT-2',\n",
    "    'mean_cross_entropy_diff_caduceus-ph_seqlen-131k_d_model-256_n_layer-16': 'Caduceus-PH',\n",
    "    'mean_cross_entropy_diff_caduceus-ps_seqlen-131k_d_model-256_n_layer-16': 'Caduceus-PS',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-multi-species': 'NT-2.5B-Multi',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-2.5b-1000g': 'NT-2.5B-1000G',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-500m-human-ref': 'NT-500M-Ref',\n",
    "    'mean_cross_entropy_diff_nucleotide-transformer-v2-500m-multi-species': 'NT-V2-500M-Multi',\n",
    "    \n",
    "    # Other models\n",
    "    'mean_cross_entropy_diff_evo2-7b': 'Evo-2',\n",
    "    'mean_cross_entropy_diff_johahi/specieslm-metazoa-upstream-k6': 'Species-LM',\n",
    "    'mean_cross_entropy_diff_songlab/gpn-animal-promoter': 'GPN-Promoter',\n",
    "    'mean_diff_evo_1_131k_base': 'Evo-1'\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_gtex_expression_data(file_path):\n",
    "    \"\"\"Load GTEx data with caching\"\"\"\n",
    "    print(f\"Loading GTEx data from {file_path}...\")\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        next(f)  # Skip headers\n",
    "        next(f)\n",
    "        df = pd.read_csv(f, sep='\\t', index_col=0)\n",
    "    return df.drop('Description', axis=1) if 'Description' in df.columns else df\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and merge all necessary datasets\"\"\"\n",
    "    print(\"Loading TFBS data...\")\n",
    "    tfbs_df = pd.read_csv(TFBS_RESULTS_PATH)\n",
    "    tfbs_df.rename({'chrom':'CHROM', 'pos':'POS', 'ref':'REF', 'alt':'ALT', 'gene':'GENE', 'species':'SPECIES'}, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"Loading old TFBS data with TF information...\")\n",
    "    old_tfbs = pd.read_csv(OLD_TFBS_PATH)\n",
    "    old_tfbs.rename({'chrom':'CHROM', 'pos':'POS', 'ref':'REF', 'alt':'ALT', 'gene':'GENE', 'species':'SPECIES'}, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"Merging TFBS datasets...\")\n",
    "    merge_columns = ['CHROM', 'POS', 'ALT', 'REF', 'GENE', 'SPECIES']\n",
    "    merge_columns = [col for col in merge_columns if col in tfbs_df.columns and col in old_tfbs.columns]\n",
    "    tfbs_df = tfbs_df.merge(old_tfbs, on=merge_columns, how='inner')\n",
    "    \n",
    "    print(\"Loading evolutionary rates data...\")\n",
    "    evo_rates = pd.read_csv(ORTHODB_RATES_PATH)\n",
    "    evo_rates['GENE'] = evo_rates['GENE'].str.lower()\n",
    "    evo_rates.dropna(inplace=True)\n",
    "    \n",
    "    print(\"Merging TFBS and evolutionary rates...\")\n",
    "    merged_df = tfbs_df.merge(evo_rates, on='GENE')\n",
    "    \n",
    "    # Rename columns for clarity (only rename what needs renaming)\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # Only rename ar_forward_llr_no_ablation to LOL-EVE if LOL-EVE doesn't already exist\n",
    "    if 'ar_forward_llr_no_ablation' in merged_df.columns and 'LOL-EVE' not in merged_df.columns:\n",
    "        column_mapping['ar_forward_llr_no_ablation'] = 'LOL-EVE'\n",
    "    \n",
    "    # Rename evolutionary rate column if it exists\n",
    "    if 'Mammalia_Evo_Rate' in merged_df.columns:\n",
    "        column_mapping['Mammalia_Evo_Rate'] = 'Mammalian_Constraint'\n",
    "    \n",
    "    if column_mapping:\n",
    "        merged_df.rename(columns=column_mapping, inplace=True)\n",
    "        print(f\"Renamed columns: {column_mapping}\")\n",
    "    \n",
    "    # Use the existing LOL-EVE column if ar_forward_llr_no_ablation doesn't exist\n",
    "    if 'LOL-EVE' not in merged_df.columns and 'ar_forward_llr_no_ablation' not in merged_df.columns:\n",
    "        print(\"Warning: No LOL-EVE column found!\")\n",
    "    \n",
    "    print(\"Loading training genes...\")\n",
    "    training_genes_df = pd.read_table(TRAINING_GENES_PATH, header=None)\n",
    "    training_genes_df[3] = training_genes_df[3].apply(lambda x: x.split('promoter_')[1])\n",
    "    training_genes = training_genes_df[3].unique()\n",
    "    \n",
    "    print(\"Loading expression data...\")\n",
    "    expression_data = load_gtex_expression_data(GTEX_FILE)\n",
    "    \n",
    "    # Load gene mapping\n",
    "    biomart_df = pd.read_csv(BIOMART_PATH, sep='\\t', usecols=['Gene stable ID', 'Gene name'])\n",
    "    gene_map = dict(zip(biomart_df['Gene stable ID'], biomart_df['Gene name']))\n",
    "    \n",
    "    # Map gene names and calculate expression variability\n",
    "    def get_gene_name(ensembl_id):\n",
    "        base_id = ensembl_id.split('.')[0]\n",
    "        return gene_map.get(base_id, ensembl_id)\n",
    "    \n",
    "    expression_data.index = expression_data.index.map(get_gene_name)\n",
    "    \n",
    "    # Calculate coefficient of variation\n",
    "    mean = expression_data.mean(axis=1)\n",
    "    std = expression_data.std(axis=1)\n",
    "    cv = std / mean\n",
    "    \n",
    "    # Create expression dataframe\n",
    "    cv_df = cv.reset_index()\n",
    "    cv_df.columns = ['Gene', 'Expression']\n",
    "    cv_df = cv_df[cv_df['Gene'].str.lower().isin([g.lower() for g in training_genes])]\n",
    "    \n",
    "    return merged_df, cv_df\n",
    "\n",
    "def create_gene_categories(merged_df, cv_df, percentile=25):\n",
    "    \"\"\"Create gene categories based on constraint and expression variability\"\"\"\n",
    "    # Prepare analysis dataframe\n",
    "    analysis_df = merged_df[['GENE', 'Mammalian_Constraint']].drop_duplicates()\n",
    "    analysis_df['GENE'] = analysis_df['GENE'].str.lower()\n",
    "    cv_df['Gene'] = cv_df['Gene'].str.lower()\n",
    "    analysis_df = analysis_df.merge(cv_df, left_on='GENE', right_on='Gene', how='inner')\n",
    "    \n",
    "    # Calculate thresholds\n",
    "    constraint_high = np.percentile(analysis_df['Mammalian_Constraint'], percentile)\n",
    "    constraint_low = np.percentile(analysis_df['Mammalian_Constraint'], 100-percentile)\n",
    "    expr_var_high = np.percentile(analysis_df['Expression'], 100-percentile)\n",
    "    expr_var_low = np.percentile(analysis_df['Expression'], percentile)\n",
    "    \n",
    "    # Create masks\n",
    "    high_constraint = analysis_df['Mammalian_Constraint'] <= constraint_high\n",
    "    low_constraint = analysis_df['Mammalian_Constraint'] >= constraint_low\n",
    "    high_variability = analysis_df['Expression'] >= expr_var_high\n",
    "    low_variability = analysis_df['Expression'] <= expr_var_low\n",
    "    \n",
    "    return {\n",
    "        'high_constraint_low_variability': set(analysis_df[high_constraint & low_variability]['GENE']),\n",
    "        'low_constraint_high_variability': set(analysis_df[low_constraint & high_variability]['GENE'])\n",
    "    }\n",
    "\n",
    "def compute_delta_accuracy_across_percentiles(merged_df, cv_df, score_columns, percentiles=range(15, 45)):\n",
    "    \"\"\"Compute delta accuracy across different percentile thresholds\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        print(f\"Processing percentile {pct}...\")\n",
    "        categories = create_gene_categories(merged_df, cv_df, percentile=pct)\n",
    "        group1_genes = categories['high_constraint_low_variability']\n",
    "        group2_genes = categories['low_constraint_high_variability']\n",
    "        \n",
    "        if len(group1_genes) < 10 or len(group2_genes) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Get common TFs\n",
    "        group1_tfs = set(merged_df[merged_df.GENE.isin(group1_genes)]['TF'])\n",
    "        group2_tfs = set(merged_df[merged_df.GENE.isin(group2_genes)]['TF'])\n",
    "        common_tfs = group1_tfs.intersection(group2_tfs)\n",
    "        \n",
    "        for tf in common_tfs:\n",
    "            tf_df = merged_df[merged_df['TF'] == tf]\n",
    "            available_g1 = set(tf_df[tf_df.GENE.isin(group1_genes)]['GENE'])\n",
    "            available_g2 = set(tf_df[tf_df.GENE.isin(group2_genes)]['GENE'])\n",
    "            \n",
    "            n_samples = min(len(available_g1), len(available_g2))\n",
    "            if n_samples < 5:\n",
    "                continue\n",
    "                \n",
    "            # Sample equal numbers from each group\n",
    "            sampled_g1 = np.random.choice(list(available_g1), n_samples, replace=False)\n",
    "            sampled_g2 = np.random.choice(list(available_g2), n_samples, replace=False)\n",
    "            \n",
    "            g1_data = tf_df[tf_df.GENE.isin(sampled_g1)]\n",
    "            g2_data = tf_df[tf_df.GENE.isin(sampled_g2)]\n",
    "            \n",
    "            for score_col in score_columns:\n",
    "                if score_col not in g1_data.columns or score_col not in g2_data.columns:\n",
    "                    continue\n",
    "                    \n",
    "                g1_scores = g1_data[score_col].values\n",
    "                g2_scores = g2_data[score_col].values\n",
    "                \n",
    "                if np.isnan(g1_scores).any() or np.isnan(g2_scores).any():\n",
    "                    continue\n",
    "                \n",
    "                # Calculate if group2 > group1 (success)\n",
    "                success = np.mean(g2_scores) > np.mean(g1_scores)\n",
    "                \n",
    "                results.append({\n",
    "                    'percentile': pct,\n",
    "                    'tf': tf,\n",
    "                    'model': score_col,\n",
    "                    'success': success,\n",
    "                    'group1_mean': np.mean(g1_scores),\n",
    "                    'group2_mean': np.mean(g2_scores),\n",
    "                    'n_samples': n_samples\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main analysis workflow\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load data\n",
    "    merged_df, cv_df = load_data()\n",
    "    \n",
    "    # Filter to available model columns\n",
    "    available_columns = [col for col in model_columns if col in merged_df.columns]\n",
    "    print(f\"Found {len(available_columns)} model columns in data\")\n",
    "    \n",
    "    # Compute delta accuracy across percentiles\n",
    "    print(\"Computing delta accuracy across percentiles...\")\n",
    "    raw_results = compute_delta_accuracy_across_percentiles(merged_df, cv_df, available_columns)\n",
    "    \n",
    "    # Save raw results\n",
    "    raw_results.to_csv(f'{OUTPUT_DIR}/raw_diagonal_results.csv', index=False)\n",
    "    print(f\"Raw results saved to {OUTPUT_DIR}/raw_diagonal_results.csv\")\n",
    "    \n",
    "    # Aggregate results by model and percentile\n",
    "    agg_results = []\n",
    "    for (model, pct), group in raw_results.groupby(['model', 'percentile']):\n",
    "        success_rate = group['success'].mean() * 100\n",
    "        delta_accuracy = success_rate - 50\n",
    "        agg_results.append({\n",
    "            'model': model,\n",
    "            'percentile': pct,\n",
    "            'delta_accuracy': delta_accuracy,\n",
    "            'n_comparisons': len(group)\n",
    "        })\n",
    "    \n",
    "    df_agg = pd.DataFrame(agg_results)\n",
    "    \n",
    "    # Aggregate across all percentiles per model\n",
    "    model_summary = (\n",
    "        df_agg.groupby('model')['delta_accuracy']\n",
    "        .agg(['mean', 'std', 'count'])\n",
    "        .reset_index()\n",
    "    )\n",
    "    model_summary['sem'] = model_summary['std'] / np.sqrt(model_summary['count'])\n",
    "    model_summary = model_summary.sort_values('mean', ascending=False)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    bars = plt.bar(\n",
    "        x=np.arange(len(model_summary)),\n",
    "        height=model_summary['mean'],\n",
    "        yerr=model_summary['sem'],\n",
    "        capsize=5,\n",
    "        color=[model_colors.get(m, '#7f7f7f') for m in model_summary['model']],\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Removed value annotations for cleaner appearance\n",
    "    \n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    plt.xticks(\n",
    "        ticks=np.arange(len(model_summary)),\n",
    "        labels=[model_display_names.get(m, m) for m in model_summary['model']],\n",
    "        rotation=45,\n",
    "        ha='right'\n",
    "    )\n",
    "    plt.ylabel('Mean Δ-Accuracy (%)')\n",
    "    plt.title('Average Δ-Accuracy Across All Percentiles\\nwith SEM Error Bars')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(f'{OUTPUT_DIR}/delta_accuracy_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f'{OUTPUT_DIR}/delta_accuracy_plot.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save summary results\n",
    "    model_summary.to_csv(f'{OUTPUT_DIR}/model_summary_results.csv', index=False)\n",
    "    df_agg.to_csv(f'{OUTPUT_DIR}/percentile_results.csv', index=False)\n",
    "    \n",
    "    print(f\"Analysis complete! Results saved to {OUTPUT_DIR}/\")\n",
    "    return raw_results, model_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_results, model_summary = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courtney_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
